{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import LeakyReLU, LSTM, Dropout, TimeDistributed\n",
    "from tensorflow.python.keras.layers.merge import _Merge\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAMPLE_INTERVAL = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2000\n",
    "d_loss_values, g_loss_values = list(), list()\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between\n",
    "    real and generated trajectory samples\"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((1, 144, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class WGANGP(keras.Model, ABC):\n",
    "    def __init__(self):\n",
    "        super(WGANGP, self).__init__()\n",
    "        self.max_length = 144\n",
    "        self.features = 1\n",
    "        self.traj_shape = (self.max_length, self.features)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_discriminator = 5\n",
    "        self.gp_weight = 10\n",
    "\n",
    "        # Build the generator and discriminator\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGANGP, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, real_trajectories, fake_trajectories):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated trajectory\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated trajectory\n",
    "        interpolated_traj = RandomWeightedAverage()([real_trajectories, fake_trajectories])\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_traj)\n",
    "            # 1. Get the discriminator output for this interpolated trajectory.\n",
    "            pred = self.discriminator(interpolated_traj, training=True)\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated trajectory.\n",
    "        grads = gp_tape.gradient(pred, [interpolated_traj])[0]\n",
    "\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "        return gp\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.traj_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.traj_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        traj = model(noise)\n",
    "\n",
    "        return Model(noise, traj)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(LSTM(128, input_shape=self.traj_shape, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(128, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(TimeDistributed(Dense(128)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(TimeDistributed(Dense(64)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(TimeDistributed(Dense(1, activation='tanh')))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        traj = Input(shape=self.traj_shape)\n",
    "        validity = model(traj)\n",
    "\n",
    "        return Model(traj, validity)\n",
    "\n",
    "    def train_step(self, real_trajectories):\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        for _ in range(self.n_discriminator):\n",
    "            # Get the latent vector\n",
    "            noise = tf.random.normal((tf.shape(real_trajectories)[0], self.latent_dim), 0, 1)\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake trajectories from the latent vector\n",
    "                fake_trajectories = self.generator(noise, training=True)\n",
    "                # Get the logits for the fake trajectories\n",
    "                fake_logits = self.discriminator(fake_trajectories, training=True)\n",
    "                # Get the logits for the real trajectories\n",
    "                real_logits = self.discriminator(real_trajectories, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real trajectory logits\n",
    "                d_cost = self.d_loss_fn(real_traj=real_logits, fake_traj=fake_logits)\n",
    "\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(real_trajectories, fake_trajectories)\n",
    "\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "                d_loss_values.append(d_loss)\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "        print(\"discriminator training done\")\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        noise = tf.random.normal((tf.shape(real_trajectories)[0], self.latent_dim), 0, 1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake trajectories using the generator\n",
    "            generated_trajectories = self.generator(noise, training=True)\n",
    "            # Get the discriminator logits for fake trajectories\n",
    "            gen_traj_logits = self.discriminator(generated_trajectories, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_traj_logits)\n",
    "            g_loss_values.append(g_loss)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        print(\"gen done\")\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "# Define the loss function for the discriminator.\n",
    "def discriminator_loss(real_traj, fake_traj):\n",
    "    real_loss = tf.reduce_mean(real_traj)\n",
    "    fake_loss = tf.reduce_mean(fake_traj)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss function for the generator.\n",
    "def generator_loss(fake_traj):\n",
    "    return -tf.reduce_mean(fake_traj)\n",
    "\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % SAMPLE_INTERVAL == 0:\n",
    "            self.model.generator.save_weights(\"G_model_\" + str(epoch) + \".h5\")\n",
    "            self.model.discriminator.save_weights(\"D_model_\" + str(epoch) + \".h5\")\n",
    "\n",
    "\n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d_loss, g_loss):\n",
    "    # plot history\n",
    "    pyplot.plot(d_loss, label='discri')\n",
    "    pyplot.plot(g_loss, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('plot_line_plot_loss.png')\n",
    "    pyplot.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    callback = GANMonitor()\n",
    "    wgan = WGANGP()\n",
    "\n",
    "    generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
    "    discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
    "    wgan.compile(\n",
    "        d_optimizer=discriminator_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=discriminator_loss,\n",
    "    )\n",
    "        # Training data\n",
    "    X_train = np.load('/content/drive/MyDrive/train.npy', allow_pickle=True)\n",
    "    wgan.fit(X_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callback])\n",
    "    plot_history(d_loss_values, g_loss_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
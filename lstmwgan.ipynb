{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.layers import LeakyReLU, LSTM, CuDNNLSTM\n",
    "from tensorflow.python.keras.layers.merge import _Merge\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAMPLE_INTERVAL = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2000\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated trajectory samples\"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((1, 144, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class WGANGP(keras.Model, ABC):\n",
    "    def __init__(self):\n",
    "        super(WGANGP, self).__init__()\n",
    "        self.max_length = 144\n",
    "        self.features = 1\n",
    "        self.traj_shape = (self.max_length, self.features)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_discriminator = 5\n",
    "        self.gp_weight = 10\n",
    "\n",
    "        # Build the generator and discriminator\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGANGP, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, real_trajectories, fake_trajectories):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated trajectory\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated trajectory\n",
    "        interpolated_traj = RandomWeightedAverage()([real_trajectories, fake_trajectories])\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_traj)\n",
    "            # 1. Get the discriminator output for this interpolated trajectory.\n",
    "            pred = self.discriminator(interpolated_traj, training=True)\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated trajectory.\n",
    "        grads = gp_tape.gradient(pred, [interpolated_traj])[0]\n",
    "\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "        return gp\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.traj_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.traj_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        traj = model(noise)\n",
    "\n",
    "        return Model(noise, traj)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(CuDNNLSTM(512, input_shape=self.traj_shape, return_sequences=True))\n",
    "        model.add(Bidirectional(CuDNNLSTM(512)))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        traj = Input(shape=self.traj_shape)\n",
    "        validity = model(traj)\n",
    "\n",
    "        return Model(traj, validity)\n",
    "\n",
    "    def train_step(self, real_trajectories):\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for _ in range(self.n_discriminator):\n",
    "            # Get the latent vector\n",
    "            noise = tf.random.normal((tf.shape(real_trajectories)[0], self.latent_dim), 0, 1)\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake trajectories from the latent vector\n",
    "                fake_trajectories = self.generator(noise, training=True)\n",
    "                # Get the logits for the fake trajectories\n",
    "                fake_logits = self.discriminator(fake_trajectories, training=True)\n",
    "                # Get the logits for the real trajectories\n",
    "                real_logits = self.discriminator(real_trajectories, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real trajectory logits\n",
    "                d_cost = self.d_loss_fn(real_traj=real_logits, fake_traj=fake_logits)\n",
    "\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(real_trajectories, fake_trajectories)\n",
    "\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "        print(\"discriminator training done\")\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        noise = tf.random.normal((tf.shape(real_trajectories)[0], self.latent_dim), 0, 1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake trajectories using the generator\n",
    "            generated_trajectories = self.generator(noise, training=True)\n",
    "            # Get the discriminator logits for fake trajectories\n",
    "            gen_traj_logits = self.discriminator(generated_trajectories, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_traj_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        print(\"gen done\")\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "# Define the loss function for the discriminator.\n",
    "def discriminator_loss(real_traj, fake_traj):\n",
    "    real_loss = tf.reduce_mean(real_traj)\n",
    "    fake_loss = tf.reduce_mean(fake_traj)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss function for the generator.\n",
    "def generator_loss(fake_traj):\n",
    "    return -tf.reduce_mean(fake_traj)\n",
    "\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % SAMPLE_INTERVAL == 0:\n",
    "            self.model.generator.save_weights(\"parameters/G_model_\" + str(epoch) + \".h5\")\n",
    "            self.model.discriminator.save_weights(\"parameters/D_model_\" + str(epoch) + \".h5\")\n",
    "\n",
    "class TrainingPlot(keras.callbacks.Callback):\n",
    "\n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.acc = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.logs = []\n",
    "\n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        self.logs.append(logs)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "\n",
    "        # Before plotting ensure at least 2 epochs have passed\n",
    "        if len(self.losses) > 1:\n",
    "            # Clear the previous plot\n",
    "            clear_output(wait=True)\n",
    "            N = np.arange(0, len(self.losses))\n",
    "\n",
    "            # You can chose the style of your preference\n",
    "            # print(plt.style.available) to see the available options\n",
    "            plt.style.use(\"seaborn\")\n",
    "\n",
    "            # Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "            plt.figure()\n",
    "            plt.plot(N, self.losses, label=\"train_loss\")\n",
    "            plt.plot(N, self.acc, label=\"train_acc\")\n",
    "            plt.plot(N, self.val_losses, label=\"val_loss\")\n",
    "            plt.plot(N, self.val_acc, label=\"val_acc\")\n",
    "            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(epoch))\n",
    "            plt.xlabel(\"Epoch #\")\n",
    "            plt.ylabel(\"Loss/Accuracy\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    callback = GANMonitor()\n",
    "    plot_losses = TrainingPlot()\n",
    "    wgan = WGANGP()\n",
    "\n",
    "    generator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "    discriminator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "    wgan.compile(\n",
    "        d_optimizer=discriminator_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=discriminator_loss,\n",
    "    )\n",
    "        # Training data\n",
    "    X_train = np.load('/content/drive/MyDrive/train.npy', allow_pickle=True)\n",
    "    wgan.fit(X_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callback])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}